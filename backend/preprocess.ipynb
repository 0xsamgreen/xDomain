{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading path data...\n",
      "Finished loading path data!\n",
      "Searching for unique paths in `opps_df` archive...\n",
      "Finsihed searching for unique paths in `opps_df` archive!\n",
      "Collect statistics about which chains, DEXs, and tokens are involved in the opportunities.\n",
      "Finding valid opportunities\n",
      "Saving variables...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Extracts details from the metadata associated with a swap ID.\n",
    "def getSwapDetails(swap_metadata):\n",
    "    chain = swap_metadata[\"chain\"].iloc[0]\n",
    "    dex = swap_metadata[\"dex\"].iloc[0]\n",
    "    token_in = swap_metadata[\"token_in_symbol\"].iloc[0]\n",
    "    token_out = swap_metadata[\"token_out_symbol\"].iloc[0]\n",
    "    return chain, dex, token_in, token_out\n",
    "\n",
    "\n",
    "# Expands a list of swap IDs into a list of dictionaries with the metadata associated\n",
    "#   with each swap ID.\n",
    "def swapsIDsToPath(swap_ids):\n",
    "    details = []\n",
    "    for id in swap_ids:\n",
    "        if id == 0:\n",
    "            continue\n",
    "\n",
    "        chain, dex, token_in, token_out = getSwapDetails(\n",
    "            opps_metadata_df.loc[opps_metadata_df[\"swap_id\"] == id]\n",
    "        )\n",
    "\n",
    "        details.append(\n",
    "            {\"chain\": chain, \"dex\": dex, \"token_in\": token_in, \"token_out\": token_out}\n",
    "        )\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "# Converts a path from a csv string to a list.\n",
    "def convertStringToList(path):\n",
    "    return [int(float(k)) for k in path[1:-1].split(\",\")]\n",
    "\n",
    "print(\"Loading path data...\")\n",
    "opps_df = pd.read_csv(\"assets/opps_cleaned-2023-3-19.csv\").reset_index(drop=True)\n",
    "opps_metadata_df = pd.read_csv(\"assets/opps_metadata-2023-3-19.csv\").reset_index(drop=True)\n",
    "print(\"Finished loading path data!\")\n",
    "\n",
    "print(\"Searching for unique paths in `opps_df` archive...\")\n",
    "unique_paths = opps_df[\n",
    "    [\"swap_1_id\", \"swap_2_id\", \"swap_3_id\", \"swap_4_id\"]\n",
    "].drop_duplicates()\n",
    "unique_paths.reset_index(inplace=True)\n",
    "# print(unique_paths)\n",
    "print(\"Finsihed searching for unique paths in `opps_df` archive!\")\n",
    "\n",
    "print(\n",
    "    \"Collect statistics about which chains, DEXs, and tokens are involved in the opportunities.\"\n",
    ")\n",
    "stats = {}\n",
    "for _, key in unique_paths.iterrows():\n",
    "    # A `swap_id` corresponds to a unique pair contract. It is unique to a specific\n",
    "    #   chain, DEX, and token pair.\n",
    "    for swap_id in [\n",
    "        key[\"swap_1_id\"],\n",
    "        key[\"swap_2_id\"],\n",
    "        key[\"swap_3_id\"],\n",
    "        key[\"swap_4_id\"],\n",
    "    ]:\n",
    "        if swap_id == 0.0:\n",
    "            continue\n",
    "\n",
    "        metadata = opps_metadata_df[opps_metadata_df[\"swap_id\"] == swap_id]\n",
    "        chain = metadata[\"chain\"].iloc[0]\n",
    "\n",
    "        if chain not in stats.keys():\n",
    "            stats[chain] = {\"dexes\": [], \"tokens\": []}\n",
    "\n",
    "        dex = metadata[\"dex\"].iloc[0]\n",
    "        if dex not in stats[chain][\"tokens\"]:\n",
    "            stats[chain][\"tokens\"].append(metadata[\"dex\"].iloc[0])\n",
    "\n",
    "        token = metadata[\"token_in_symbol\"].iloc[0]\n",
    "        if token not in stats[chain][\"dexes\"]:\n",
    "            stats[chain][\"dexes\"].append(metadata[\"token_in_symbol\"].iloc[0])\n",
    "\n",
    "# A token pair is often involved in multiple opportunities for a given block.\n",
    "#   Ignore \"inferior\" opportunities that occur at the same timestamp as better ones.\n",
    "\n",
    "# Get unique timestamps from `opps_df` archive.\n",
    "unique_timestamps = opps_df[\"timestamp\"].unique()\n",
    "\n",
    "# Track the valid opportunities at each unique timestamp.\n",
    "print(\"Finding valid opportunities\")\n",
    "valid_opps = {}\n",
    "for i, timestamp in enumerate(unique_timestamps):\n",
    "    if np.isnan(timestamp):\n",
    "        # This timestamp is invalid.\n",
    "        # print(\"found nan!\")\n",
    "        continue\n",
    "\n",
    "    # Get opportunities that happen at the same time. Sort them by profit.\n",
    "    opps_simultaneous = opps_df[opps_df[\"timestamp\"] == timestamp].sort_values(\n",
    "        by=\"profit_usd\", ascending=False\n",
    "    )\n",
    "\n",
    "    valid_opps[timestamp] = []\n",
    "\n",
    "    # Track the swap IDs used at this timestamp. Only keep the opportunities that don't have\n",
    "    #   conflicting swap IDs.\n",
    "    swap_ids_used = set()\n",
    "    for j, opp in opps_simultaneous.iterrows():\n",
    "        valid = 1\n",
    "        swap_ids = opp[[\"swap_1_id\", \"swap_2_id\", \"swap_3_id\", \"swap_4_id\"]].to_list()\n",
    "        for id in swap_ids:\n",
    "            # `swap_3_id` and `swap_4_id` could be 0 if not used. No need to track those.\n",
    "            if id == 0:\n",
    "                break\n",
    "            # This ID is not used yet, so it is valid.\n",
    "            elif id not in swap_ids_used:\n",
    "                continue\n",
    "            # This ID is used already, and it is conflicting with a greater opportunity.\n",
    "            else:\n",
    "                # No need to look further because this opportunity is inferior.\n",
    "                valid = 0\n",
    "                break\n",
    "\n",
    "        # Add the swap ID to the set of used ones.\n",
    "        if valid:\n",
    "            swap_ids_used = swap_ids_used.union(set(swap_ids))\n",
    "            valid_opps[timestamp].append(opp)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Block rate (in seconds) for each chain that we track.\n",
    "block_rate = {\n",
    "    'polygon':2,\n",
    "    'ethereum':12,\n",
    "    'bsc':3,\n",
    "    'fantom':3,\n",
    "    'avalanche':1,\n",
    "    'gnosis':5,\n",
    "    'arbitrum':1,\n",
    "    'optimism':2\n",
    "}\n",
    "\n",
    "# Track the timestamps, profits, and block numbers where each opportunity occurs. \n",
    "path_data = {}\n",
    "for timestamp, opps in valid_opps.items():\n",
    "    for opp in opps: \n",
    "        path = str(opp[['swap_1_id', 'swap_2_id', 'swap_3_id', 'swap_4_id']].to_list())\n",
    "\n",
    "        # Maps chains to the block numbers for the current opportunity\n",
    "        block_numbers = opp[['swap_1_block', 'swap_2_block', 'swap_3_block', 'swap_4_block']].tolist()\n",
    "        \n",
    "        if path not in path_data:\n",
    "            path_data[path] = {\n",
    "                'timestamps': [timestamp], \n",
    "                'profit_usd': [opp['profit_usd']],\n",
    "                'block_numbers': [block_numbers]\n",
    "            }\n",
    "        else:\n",
    "            path_data[path]['timestamps'].append(timestamp)\n",
    "            path_data[path]['profit_usd'].append(opp['profit_usd'])\n",
    "            path_data[path]['block_numbers'].append(block_numbers)\n",
    "\n",
    "# Sort the `path_data` by timestamps. Basically gets us a time series version of the data.\n",
    "for path, data in path_data.items():\n",
    "    timestamps = data['timestamps']\n",
    "    profits = data['profit_usd']\n",
    "    path_data[path]['timestamps'], path_data[path]['profits_usd'] = (\n",
    "        list(t) for t in zip(*sorted(zip(timestamps, profits)))\n",
    "    )\n",
    "\n",
    "# Stores opportunity clusters, which are clustered by relative profit and time.\n",
    "unique_opps_profit_time = {}\n",
    "\n",
    "# Loop through each path and assign its opportunities to a cluster.\n",
    "for path, data in path_data.items():\n",
    "\n",
    "    # Initialize the clustering dictionary for this path.\n",
    "    unique_opps_profit_time[path] = {\n",
    "        'profits':[],\n",
    "        'timestamps': []\n",
    "    }\n",
    "\n",
    "    # Get the swap IDs used in the path.\n",
    "    swap_ids = convertStringToList(path) \n",
    "    \n",
    "    # Find the slowest chain in the path.\n",
    "    slowest_block = 0\n",
    "    for id in swap_ids:\n",
    "        if id == 0:\n",
    "            break\n",
    "\n",
    "        # Get the chain name corresponding to the swap ID: Ethereum, Polygon, etc.\n",
    "        chain_name = opps_metadata_df[opps_metadata_df['swap_id'] == id]['chain'].iloc[0]\n",
    "\n",
    "        # Check if the current chain is the slowest in the path.\n",
    "        if block_rate[chain_name] > slowest_block:\n",
    "            slowest_block = block_rate[chain_name]\n",
    "            slowest_chain = chain_name\n",
    "\n",
    "    # Find all the opportunities and how long they persist when clustered.\n",
    "    timestamps = data['timestamps']\n",
    "    profits = data['profit_usd']\n",
    "    profit_reference = profits[0] \n",
    "    time_reference = timestamps[0]\n",
    "\n",
    "    # Variables used to track the current opportunity cluster.\n",
    "    cluster_profits = [profits[0]]\n",
    "    cluster_timestamps = [timestamps[0]]\n",
    "\n",
    "    # Assign all opportunities in this path to a cluster.\n",
    "    for i in range(1, len(timestamps)):\n",
    "        current_profit = profits[i]\n",
    "        current_time = timestamps[i]\n",
    "        \n",
    "        # If this opportunity is too different from the last opportunity in the current cluster, \n",
    "        #  store the current cluster and start a new one.\n",
    "\n",
    "        # Current opportunity can be at most 10% different from the last opportunity in the cluster.\n",
    "        propotion = current_profit / profit_reference\n",
    "        max_deviation = .1\n",
    "\n",
    "        # Current opportunity can be at most 100 seconds later than the last opportunity in the cluster.\n",
    "        delta = current_time - time_reference \n",
    "        max_delta = 100\n",
    "\n",
    "        # The following `if` logic runs whenever we need to store a cluster\n",
    "        if propotion < (1 - max_deviation) or propotion > (1 + max_deviation) or delta > max_delta: \n",
    "            # Store the details of the current opportunity\n",
    "            unique_opps_profit_time[path]['profits'].append(cluster_profits)\n",
    "            unique_opps_profit_time[path]['timestamps'].append(cluster_timestamps)\n",
    "\n",
    "            # Reset variables used to track next opportunity\n",
    "            cluster_profits = [profits[i]]\n",
    "            cluster_timestamps = [timestamps[i]]\n",
    "        # The following logic runs when we assign an opportunity to an existing cluster\n",
    "        else:\n",
    "            cluster_profits.append(profits[i])\n",
    "            cluster_timestamps.append(timestamps[i])\n",
    "        \n",
    "        # Reset the reference points\n",
    "        profit_reference = current_profit\n",
    "        time_reference = current_time\n",
    "\n",
    "# Calculate more metadata for each opportunity cluster\n",
    "for key, metadata in unique_opps_profit_time.items():\n",
    "    max_profit_sum = 0\n",
    "    min_profit_sum = 0\n",
    "    opportunity_duration_sum = 0\n",
    "    for timestamps, profits in zip(metadata['timestamps'], metadata['profits']):\n",
    "        max_profit_sum += max(profits) \n",
    "        min_profit_sum += min(profits) \n",
    "        opportunity_duration_sum += len(timestamps)\n",
    "\n",
    "    unique_opps_profit_time[key]['max_profit_sum'] = max_profit_sum\n",
    "    unique_opps_profit_time[key]['min_profit_sum'] = min_profit_sum\n",
    "    unique_opps_profit_time[key]['num_opportunities'] = len(metadata['profits'])\n",
    "    unique_opps_profit_time[key]['opportunity_duration_sum'] = opportunity_duration_sum\n",
    "\n",
    "    swap_ids = convertStringToList(key)\n",
    "    unique_opps_profit_time[key]['path'] = swapsIDsToPath(swap_ids)\n",
    "    \n",
    "# Save all the variables to a binary file.\n",
    "print(\"Saving variables...\")\n",
    "with open('assets/opps.pkl', 'wb') as f:\n",
    "    pickle.dump(opps_df, f)\n",
    "\n",
    "with open('assets/opps_metadata_df.pkl', 'wb') as f:\n",
    "    pickle.dump(opps_metadata_df, f)\n",
    "\n",
    "with open('assets/path_data.pkl', 'wb') as f:\n",
    "    pickle.dump(path_data, f)\n",
    "\n",
    "with open('assets/unique_opps_profit_time.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_opps_profit_time, f)\n",
    "\n",
    "with open('assets/unique_paths.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_paths, f)\n",
    "\n",
    "with open('assets/unique_timestamps.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_timestamps, f)\n",
    "\n",
    "with open('assets/block_rate.pkl', 'wb') as f:\n",
    "    pickle.dump(block_rate, f)\n",
    "\n",
    "with open('assets/stats.pkl', 'wb') as f:\n",
    "    pickle.dump(stats, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8967a2d626e9067b8d58b6a807cc6cac7092787939b751e12b5a151fb89b5425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
